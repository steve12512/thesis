def identify_favored_classes(di_ratio_group, eod_group, dpd_group, group_name, group_labels, y_true, y_pred, sensitive_features):
    favored_classes = []
    disadvantaged_classes = []

    # Define thresholds for fairness metrics
    di_threshold = 0.8  # Demographic Parity Ratio threshold
    eod_threshold = 0.1  # Equalized Odds Difference threshold
    dpd_threshold = 0.1  # Demographic Parity Difference threshold

    for label in group_labels:
        mask = sensitive_features == label
        y_true_class = y_true[mask]
        y_pred_class = y_pred[mask]

        # Skip if there are no samples for this class
        if len(y_true_class) == 0:
            continue

        # Calculate fairness metrics for each class (e.g., Male, Female)
        eod_class = equalized_odds_difference(y_true=y_true_class, y_pred=y_pred_class, sensitive_features=sensitive_features[mask])
        dpd_class = demographic_parity_difference(y_true=y_true_class, y_pred=y_pred_class, sensitive_features=sensitive_features[mask])
        di_ratio_class = demographic_parity_ratio(y_true=y_true_class, y_pred=y_pred_class, sensitive_features=sensitive_features[mask])

        # Calculate the differences with the overall group metrics
        eod_diff = eod_class - eod_group
        dpd_diff = dpd_class - dpd_group
        di_ratio_diff = di_ratio_class - di_ratio_group

        print(f"\nFor {group_name} = {label}:")
        print(f"Demographic Parity Ratio (Class): {di_ratio_class:.4f}, Difference: {di_ratio_diff:.4f}")
        print(f"Equalized Odds Difference (Class): {eod_class:.4f}, Difference: {eod_diff:.4f}")
        print(f"Demographic Parity Difference (Class): {dpd_class:.4f}, Difference: {dpd_diff:.4f}")

        # Classify the label as favored or disadvantaged
        if di_ratio_diff > 0 and abs(eod_diff) <= eod_threshold and abs(dpd_diff) <= dpd_threshold:
            favored_classes.append(label)
        else:
            disadvantaged_classes.append(label)

    return favored_classes, disadvantaged_classes






def evaluate_fairness(y_true, y_pred, sensitive_features, group_name):
    # Calculate overall group fairness metrics
    eod_group = equalized_odds_difference(y_true=y_true, y_pred=y_pred, sensitive_features=sensitive_features)
    dpd_group = demographic_parity_difference(y_true=y_true, y_pred=y_pred, sensitive_features=sensitive_features)
    di_ratio_group = demographic_parity_ratio(y_true=y_true, y_pred=y_pred, sensitive_features=sensitive_features)

    print(f'\nGroup: {group_name}')
    print(f"Demographic Parity Ratio (Group): {di_ratio_group:.4f}")
    print(f"Equalized Odds Difference (Group): {eod_group:.4f}")
    print(f"Demographic Parity Difference (Group): {dpd_group:.4f}")

    # Get unique labels in the sensitive feature (e.g., Male, Female)
    group_labels = sensitive_features.unique()

    # Identify favored and disadvantaged classes
    favored, disadvantaged = identify_favored_classes(
        di_ratio_group, eod_group, dpd_group, group_name, group_labels, y_true, y_pred, sensitive_features
    )

    print(f"\nFavored classes for {group_name}: {favored}")
    print(f"Disadvantaged classes for {group_name}: {disadvantaged}")

